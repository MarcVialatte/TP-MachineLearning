{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483c16ce",
   "metadata": {},
   "source": [
    "# TP2 - NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47cf87b2-9b52-407d-bec6-a16c8d0501e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Chargement des données\n",
    "url = \"nlp_1.csv\"\n",
    "data = pd.read_csv(url, delimiter=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a88190-0ad2-4466-9ad5-5d369c709537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 91</th>\n",
       "      <th>Unnamed: 92</th>\n",
       "      <th>Unnamed: 93</th>\n",
       "      <th>Unnamed: 94</th>\n",
       "      <th>Unnamed: 95</th>\n",
       "      <th>Unnamed: 96</th>\n",
       "      <th>Unnamed: 97</th>\n",
       "      <th>Unnamed: 98</th>\n",
       "      <th>Unnamed: 99</th>\n",
       "      <th>Unnamed: 100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                               text  Unnamed: 2  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...         NaN   \n",
       "1    ham                      Ok lar... Joking wif u oni...         NaN   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...         NaN   \n",
       "3    ham  U dun say so early hor... U c already then say...         NaN   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...         NaN   \n",
       "\n",
       "   Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  \\\n",
       "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   Unnamed: 9  ...  Unnamed: 91  Unnamed: 92  Unnamed: 93  Unnamed: 94  \\\n",
       "0         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "1         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "2         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "3         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "4         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 95  Unnamed: 96  Unnamed: 97  Unnamed: 98  Unnamed: 99  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 100  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e31180",
   "metadata": {},
   "source": [
    "## Exercice 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bc64c",
   "metadata": {},
   "source": [
    "### Question 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a458fbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Accuracy: 0.9811151079136691\n",
      "Naive Bayes - Recall: 0.9811151079136691\n",
      "Naive Bayes - F1 Score: 0.9807792982020312\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "nb_model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_recall = recall_score(y_test, nb_predictions, average='weighted')\n",
    "nb_f1 = f1_score(y_test, nb_predictions, average='weighted')\n",
    "\n",
    "print(\"Naive Bayes - Accuracy:\", nb_accuracy)\n",
    "print(\"Naive Bayes - Recall:\", nb_recall)\n",
    "print(\"Naive Bayes - F1 Score:\", nb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d66b3",
   "metadata": {},
   "source": [
    "### Question 2: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea935528-acff-465b-82f7-1f317f508316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perceptron - Accuracy: 0.9622302158273381\n",
      "Perceptron - Recall: 0.9622302158273381\n",
      "Perceptron - F1 Score: 0.9628819100408297\n"
     ]
    }
   ],
   "source": [
    "perceptron_model = make_pipeline(CountVectorizer(), StandardScaler(with_mean=False), Perceptron())\n",
    "perceptron_model.fit(X_train, y_train)\n",
    "\n",
    "perceptron_predictions = perceptron_model.predict(X_test)\n",
    "perceptron_accuracy = accuracy_score(y_test, perceptron_predictions)\n",
    "perceptron_recall = recall_score(y_test, perceptron_predictions, average='weighted')\n",
    "perceptron_f1 = f1_score(y_test, perceptron_predictions, average='weighted')\n",
    "\n",
    "print(\"\\nPerceptron - Accuracy:\", perceptron_accuracy)\n",
    "print(\"Perceptron - Recall:\", perceptron_recall)\n",
    "print(\"Perceptron - F1 Score:\", perceptron_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b72bb",
   "metadata": {},
   "source": [
    "### Question 3: Techniques d'amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35149a0c-f307-4e48-9b5a-341e824ec147",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# - Utiliser des techniques de prétraitement telles que la suppression de stop words, la lemmatisation, etc.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# - Utiliser des techniques de prétraitement telles que la suppression de stop words, la lemmatisation, etc.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download stopwords and WordNet resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to your text data\n",
    "X_train_preprocessed = X_train.apply(preprocess_text)\n",
    "X_test_preprocessed = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c54ad1",
   "metadata": {},
   "source": [
    "### Question 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1f791-e239-4830-a91a-b1bc0d4957a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Perceptron model on preprocessed data\n",
    "perceptron_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "perceptron_predictions_cleaned = perceptron_model.predict(X_test_preprocessed)\n",
    "\n",
    "# Evaluating the model after data cleaning\n",
    "perceptron_accuracy_cleaned = accuracy_score(y_test, perceptron_predictions_cleaned)\n",
    "perceptron_recall_cleaned = recall_score(y_test, perceptron_predictions_cleaned, average='weighted')\n",
    "perceptron_f1_cleaned = f1_score(y_test, perceptron_predictions_cleaned, average='weighted')\n",
    "\n",
    "print(\"\\nPerceptron (after data cleaning) - Accuracy:\", perceptron_accuracy_cleaned)\n",
    "print(\"Perceptron (after data cleaning) - Recall:\", perceptron_recall_cleaned)\n",
    "print(\"Perceptron (after data cleaning) - F1 Score:\", perceptron_f1_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1602ea",
   "metadata": {},
   "source": [
    "## Exercice 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ef247",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c6d3c-f6ed-4218-a9db-8b9559976f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Computer_security\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#### Trouver toutes les données (par exemple, dans des balises <p>)\n",
    "data_list = [paragraph.text for paragraph in soup.find_all('p')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f49f5",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ffdf35-64c3-45d9-b8bf-553310c572f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Télécharger les stopwords (une seule fois)\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Nettoyage des données\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [ps.stem(word) for word in words if word.isalnum() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "cleaned_data = [clean_text(sentence) for sentence in data_list]\n",
    "\n",
    "# Utilisation de Word2Vec\n",
    "model = Word2Vec(sentences=cleaned_data, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebf382",
   "metadata": {},
   "source": [
    "### Question 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac00aa-3674-444c-86f8-4c760b5b8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words_attack = model.wv.most_similar('attack', topn=5)\n",
    "similar_words_protect = model.wv.most_similar('protect', topn=5)\n",
    "\n",
    "print(\"Similar words for 'attack':\", similar_words_attack)\n",
    "print(\"Similar words for 'protect':\", similar_words_protect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae24117",
   "metadata": {},
   "source": [
    "### Question 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d08169-7af5-4427-89eb-577f1c6d5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convertir la liste de vecteurs en un tableau NumPy\n",
    "vectors_array = np.array(vectors)\n",
    "\n",
    "# Réduction de dimension avec t-SNE\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "vectors_2d = tsne_model.fit_transform(vectors_array)\n",
    "\n",
    "# Tracer les points\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='blue')\n",
    "\n",
    "# Ajouter des annotations pour chaque point\n",
    "for i, word in enumerate(words_to_plot):\n",
    "    plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 2), textcoords='offset points', ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c7928",
   "metadata": {},
   "source": [
    "### Question 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d13ff6-b90f-42d3-b599-646259f1bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenez les vecteurs pour les mots \"attack\" et \"human\"\n",
    "vectors_attack = model.wv['attack'].reshape(1, -1)\n",
    "vectors_human = model.wv['human'].reshape(1, -1)\n",
    "\n",
    "# Concaténez les vecteurs\n",
    "vectors_to_plot = np.concatenate([vectors_attack, vectors_human])\n",
    "\n",
    "# Réduction de dimension avec t-SNE\n",
    "tsne_model = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=1).fit_transform(vectors_to_plot)\n",
    "\n",
    "# Tracer les points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tsne_model[:, 0], tsne_model[:, 1], c=['red', 'green'])\n",
    "\n",
    "# Ajouter des annotations pour chaque point\n",
    "plt.annotate('attack', xy=(tsne_model[0, 0], tsne_model[0, 1]), xytext=(5, 2), textcoords='offset points', ha='right', color='red')\n",
    "plt.annotate('human', xy=(tsne_model[1, 0], tsne_model[1, 1]), xytext=(5, 2), textcoords='offset points', ha='right', color='green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002a5e6",
   "metadata": {},
   "source": [
    "## Exercice 3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa96beb-af59-46d4-ba9e-2cdfd5f8c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Charger les données\n",
    "data = pd.read_csv(\"emotion_classify_dataset.csv\")\n",
    "\n",
    "# Afficher les premières lignes du DataFrame pour vérifier les données\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff14ea-b628-419e-b84a-fd206a5047a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data['Comment'], data['Emotion'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer un vecteur TF-IDF pour représenter les phrases\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Créer et entraîner un modèle de forêt aléatoire\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(train_vectors, train_labels)\n",
    "\n",
    "# Prédire les émotions sur l'ensemble de test\n",
    "predictions = classifier.predict(test_vectors)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "classification_rep = classification_report(test_labels, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
